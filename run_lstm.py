# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sWvSMWBhV64HOqkvHcLGSpizn_lMoR46
"""

# import libraries and mount the google drive

import csv
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Smart energy/Final Project/household_power_consumption.txt'

# read the text file
data = pd.read_csv(path, sep = ';', infer_datetime_format=True, low_memory=False, na_values=['nan','?'])

data2 = data.dropna(how='any')

# active energy consumed /min (in wH) = (global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3)

data2['Active_Energy_per_min'] = round(((data2['Global_active_power'].astype(float))*1000/60 - (data2['Sub_metering_1'].astype(float)) - (data2['Sub_metering_2'].astype(float)) - (data2['Sub_metering_3'].astype(float))),1)
data2.head()

import torch
import torch.nn as nn
import numpy as np

# Define the LSTM model
class LSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(LSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# Generate sample data and select a sample of the data for compute limitation
data = data2['Active_Energy_per_min'][:10000]  # Example time series data

# Normalize the data
data_max = np.max(data)
data_min = np.min(data)
data_normalized = (data - data_min) / (data_max - data_min)

# Convert data to PyTorch tensors
data_normalized = torch.FloatTensor(data_normalized).unsqueeze(0).unsqueeze(2)  # Adding batch dimension

# Define split ratio for train-test
split_ratio = 0.8
split_idx = int(len(data_normalized[0]) * split_ratio)

# Split data into train and test sets
train_data = data_normalized[:, :split_idx, :]
test_data = data_normalized[:, split_idx:, :]

# Define hyperparameters
input_size = 1  # Size of each input data point
hidden_size = 64  # Number of LSTM units
num_layers = 1  # Number of LSTM layers
output_size = 1  # Size of the output

# Instantiate the model
model = LSTM(input_size, hidden_size, num_layers, output_size)

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Train the model
num_epochs = 100
for epoch in range(num_epochs):
    # Forward pass
    outputs = model(train_data)
    loss = criterion(outputs, train_data)

    # Backward and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Predict next n values
n = len(test_data[0])
predictions = []
with torch.no_grad():
    input_seq = test_data.clone()
    for _ in range(n):
        # Predict the next value
        prediction = model(input_seq[:, -1:, :])
        predictions.append(prediction.item())

        # Append the predicted value to the input sequence for the next prediction
        input_seq = torch.cat((input_seq, prediction.unsqueeze(1)), dim=1)

# Denormalize the predictions
# predictions = np.array(predictions) * (data_max - data_min) + data_min
print("Predicted next", n, "values:", predictions)

from sklearn.metrics import mean_squared_error
import math

rmse = math.sqrt(mean_squared_error(test_data.ravel().tolist(), predictions))

print(rmse)

