# -*- coding: utf-8 -*-
"""SE_Project (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18XsvnY9SIXnSVAQnsyZ3MmGSVIAvn_V0
"""

import csv
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/SmartEnergy/household_power_consumption.txt'

data = pd.read_csv(path, sep = ';', infer_datetime_format=True, low_memory=False, na_values=['nan','?'])

data2 = data.dropna(how='any')

# active energy consumed /min (in wH) = (global_active_power*1000/60 - sub_metering_1 - sub_metering_2 - sub_metering_3)

data2['Active_Energy_per_min'] = round(((data2['Global_active_power'].astype(float))*1000/60 - (data2['Sub_metering_1'].astype(float)) - (data2['Sub_metering_2'].astype(float)) - (data2['Sub_metering_3'].astype(float))),1)
data2.head()

data3 = data2.drop(columns=['Global_active_power', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3', 'Global_reactive_power', 'Voltage', 'Global_intensity'])

data3['Date_Formatted'] = pd.to_datetime(data3['Date'], format="%d/%m/%Y")#.dt.date
data3['Time_Formatted'] = pd.to_datetime(data3['Time'], format="%H:%M:%S")#.dt.time

"""Keeping all the features"""

data4 = data3.drop(columns=['Date', 'Time'])
data4.head()

def row_to_sentence(row):

    return (
        f"The Active Energy Consumption per minute on {row['Date_Formatted'].strftime('%Y-%m-%d')} at {row['Time_Formatted'].strftime('%H:%M:%S')} "
        f"is {row['Active_Energy_per_min']}"
    )

sentences = data4.apply(row_to_sentence, axis=1)

file_path = '/content/drive/MyDrive/Smart energy/Final Project/energy_consumption_sentences2.txt'

# Writing the sentences to the file
with open(file_path, 'w') as file:
    for sentence in sentences:
        file.write(sentence + '\n')

"""Reading from file"""

file_path = '/content/energy_consumption_sentences2.txt'

sentences = []
with open(file_path, 'r') as file:
        content = file.read()

sentences = content.strip().split('\n')

# for m, sentence in enumerate(sentences):
#     print(sentence)
#     if m==6:
#       break

import torch
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn

def collate_fn(batch):
    input_ids = [item[0] for item in batch]
    attention_masks = [item[1] for item in batch]

    # Find the maximum length among all tensors
    max_len = max(len(ids) for ids in input_ids)

    # Pad each tensor to the maximum length
    input_ids = [torch.cat((ids, torch.zeros(max_len - len(ids), dtype=torch.long))) for ids in input_ids]
    attention_masks = [torch.cat((mask, torch.zeros(max_len - len(mask), dtype=torch.long))) for mask in attention_masks]

    return torch.stack(input_ids), torch.stack(attention_masks)



# Define a custom dataset class
class EnergyDataset(Dataset):
    def __init__(self, pairs, tokenizer):
        self.pairs = pairs
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        x, y = self.pairs[idx]

        # Tokenize the input-output pair
        x_encoded = self.tokenizer(x, return_tensors="pt", padding=True, truncation=True)
        y_encoded = self.tokenizer(y, return_tensors="pt", padding=True, truncation=True)

        # Combine input and output for GPT to learn sequence completion
        input_ids = torch.cat((x_encoded.input_ids, y_encoded.input_ids[:, 0:]), dim=1)
        attention_mask = torch.cat((x_encoded.attention_mask, y_encoded.attention_mask[:, 1:]), dim=1)

        return input_ids.squeeze(0), attention_mask.squeeze(0)

pairs = [(s.split(" is")[0] + " is " , s.split("is ")[1]) for s in sentences if " is" in s]

len(pairs)

# Split pairs into training and test sets
train_pairs, test_pairs = train_test_split(pairs, test_size=0.4, random_state=42)

# Load tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# Create datasets
train_dataset = EnergyDataset(train_pairs, tokenizer)
test_dataset = EnergyDataset(test_pairs, tokenizer)

# Dataloaders
train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)

len(train_dataloader)

test_dataset[0]






'''Training'''
model = GPT2LMHeadModel.from_pretrained("gpt2")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6, weight_decay=1e-4)
tokenizer.pad_token = tokenizer.eos_token
# Manual training loop
for epoch in range(1):  # Number of epochs
    model.train()
    # total_train_loss = 0

    for m, (inputs, masks) in enumerate(train_dataloader):
        inputs, masks = inputs.to(device), masks.to(device)

        optimizer.zero_grad()

        outputs = model(inputs, attention_mask=masks, labels=inputs)
        loss = outputs.loss  # Cross-Entropy Loss

        loss.backward()  # Backpropagate the loss
        optimizer.step()  # Update weights

        # total_train_loss += loss.item()

        if m%40==0:
          print(f"Epoch {epoch + 1}, Step {m}, Training Loss = {loss.item()}")
        if m>1650:
          break




'''Evaluating'''
def evaluate_predictions(model, test_dataloader, tokenizer, max_length=27):
    model.eval()  # Set the model to evaluation mode
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    predictions = []
    actuals = []

    with torch.no_grad():
        for inputs, masks in test_dataloader:
            # Move inputs and masks to the appropriate device
            inputs, masks = inputs.to(device), masks.to(device)

            # Convert inputs to lists of token IDs
            input_ids = inputs[0].tolist()

            is_token_id = 220
            tokenizer.pad_token_id = 50256
            if is_token_id in inputs[0].tolist():
                split_idx = inputs[0].tolist().index(is_token_id) + 1

                x_input = inputs[:, :split_idx]
                x_mask = masks[:, :split_idx]

                # Generate prediction
                outputs = model.generate(x_input, max_length=max_length, num_return_sequences=1, attention_mask=x_mask, pad_token_id=tokenizer.pad_token_id, temperature=59, top_k=10)

                # Decode texts
                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                actual_text = tokenizer.decode(inputs[0].tolist(), skip_special_tokens=True)

                # Record results
                predictions.append(generated_text)
                actuals.append(actual_text)

                print(f"Actual: {actual_text}")
                print(f"Generated: {generated_text}\n")
            else:
                print(f"No 'with' token found in: {tokenizer.decode(inputs[0].tolist(), skip_special_tokens=True)}")
                break

# Ensure this function integrates with your existing setup
evaluate_predictions(model, test_dataloader, tokenizer)

























